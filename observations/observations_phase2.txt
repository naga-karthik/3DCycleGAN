Information regarding the new files created, changes made and experiments performed

DATASET: A new dataset has been created, with new augmentation methods - (i) elastic deformation and (ii) contrast stretching, on top of the existing, random rotation and white noise addition.
	MRI training data - 1214 volumes	MRI testing data - 15 volumes
	CT training data - 200 volumes		

Observations:
1) torch.nn.DataParallel is not working as expected because there is a huge imbalance in GPU memory consumption. Only one GPU uses all the memory and increasing batch size causes out-of-memory error.	(Solution: torch.nn.parallel.DistributedDataParallel)
2) The distributed package works well to avoid the memory imbalance problem. On initial testing, it was observed that when running on 2 GPUs, both their memories were being used equally. 


Experiments to run:
1) Vanilla CycleGAN3D model with Adversarial, Cycle-consisteny, and Identity losses for 250 epochs
2) CycleGAN3D with perceptual loss and the remaining standard losses. (works only on Cedar's GPUs because it consumes about 30GB memory on both GPUs)


15/06/20 - CHANGELOG (new observations/remarks)

Experiments from PREVIOUS To-Do: 
(1) Ran the vanilla CycleGAN for 250 epochs, results not all good. One possibility - discriminator too strong and generator too weak (revamped the entire generator architecture and reduced the discriminator's capacity in the new experiments)
(2) CycleGAND3D with Perceptual losses only runs when the data is divided into patches. The result for this experiment was also not good at all. Possibility - Non-overlapping are kind of disjoint so the model cannot figure what the images look like at the borders of each patch. This was trained 
for 150 epochs, with no decent results

New additions:
(1) Changed the UnetGenerator's architecture completely. The original volume is downsampled 16 times at the bottle neck. The convolution blocks now have a sequence of 2 convolutions (unlike in the previous with only 1 conv layer)
(2) 2 models were created: LighterUnetGenerator3D and LighterResUnetGenerator3D (this contains skip connections). One thing is unclear as to why despite having skip connections the model training time did not witness an observable speed-up.
*(3) Crucial yet lucky observation: When using DDP, saving models should be done in such a way that all the processes do not save at the same local rank (thereby overwriting the master processes' saved data). This caused a "serialization" error which meant that the saved data was corrupted and cannot be retrived. SOLUTION: Make sure that the only the master process saved the model thereby preventing any overwriting.
**(4) Note that when running models parallely, the batch size should be greater than or equal to (>=) the number of GPUs being used. That way, it is more efficient.

Ongoing Experiments:
(1) LighterUnetGenerator with Gradient Consistency loss. On reducing the number of channels at each layer by 2, the batch size could be increased to 4, thus making it possible to run parallely with 4 GPUs.
 





