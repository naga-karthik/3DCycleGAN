A file for recording observations in PHASE-4 of experimentation with cycleGAN3D.

DATASET: the same dataset mr2ct_newCorrected which was used for the SPIE's results.

CODE CHANGES:
1) Tried running the reversible network by using RevTorch package available on GitHub. This can be found in models3D_unetReversible file. Also tried using memCNN package, the code for that is in the models3D_revnet and revnet_prev_.. files.
2) In the default (working) code, 2 notable changes made -> (i) updating the generator more frequently compared to the discriminator to check whether it improves the performance, and (ii) for some reason, there was no InstanceNorm layer in the PatchGAN discriminator. That layer has been added before each leakyReLU layer.

ONGOING EXPERIMENTS:
1) The 5th version of the "working" architecture but explicitly imbalancing the G and D updates. n_gen = 5 in this case. Also, the hyperparameter values for the cycle-consistency is now 3.0 (instead of 10.0 earlier)
	folder: 

OBSERVATIONS:
(1) The results from the RevTorch version of the reversible architecture did not provide good results. Need to investigate further. It's the same problem of all the generated test volumes being the same.
(2) Not able to run memCNN version of reversible architecture because of NaN problem. Right from the 1st volume of the 1st epoch, the loss values are NaNs. Need to tweak the architecture to see what the problem is.


---------- [9/11/20] UPDATES and TO-DOs -----------
(1) The switch from PyTorch to PyTorch-Lighntning has worked. Now 16-bit training is also possible. On first sight, the time needed for an epoch is lesser and the importantly, the GPU memory consumption is also reduced by quite a bit.
TO-DO --> (DONE) (2) Increase number of feature maps and increase the depth of the model to see if the translation results have improved. 
TO-DO --> (3) Try Bayesian U-Net model by adding dropout layers in between. Read literature first.

[13/11/20] -- Lightning experiments
Observations:
(1) Increased the feature maps from [20, 40, 80, 160] to [32, 64, 128, 256]. But, decreasing Lambda_cycle from 10.0 to 3.0 was not good. There's no proper shape/structure in the results.
(2) The problem of all-test-volumes-being-the-same has NOT occured (whew!) - all volumes look different but not as good. 

--------TO-DOs----------
(1) Crank up both cycle-consistency and gradient consistency losses by doubling them. SAVE THOSE HYPERPARAMETERS ON TENSORBOARD i.e Lambda_cycle and Lambda_gradcon
(2) COPIED FROM PREVIOUS TO-DO --> Try Bayesian U-Net model by adding dropout layers in between. Read literature first.






